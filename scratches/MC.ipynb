{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b88911",
   "metadata": {},
   "source": [
    "## Skew-symmetric Matrix Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35cde403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517211a7",
   "metadata": {},
   "source": [
    "### Algorithm 1: Gradient Descent\n",
    "\n",
    "Let $M$ dentoe the observed matrix, $\\Omega$ denote the indicator matrix (0:observed;1:observed)\n",
    "\n",
    "Any rank-r (note r must be an even numeber) skew-symmetric matrix $M$ admits a linear factorization $M = AB'-BA'$, where $A,B \\in R^{n\\times {r\\over2}}$.\n",
    "\n",
    "This algorithm minimizes $\\frac{1}{2p}||P_\\Omega(M-(AB'-B'A))||^2_F + {1\\over 4}||A'A-B'B||^2_F + {1\\over 4}||A'B+B'A||^2_F$ via gradient descent. \n",
    "\n",
    "Initialization is based on schur decoposition. Let $(A_0,B_0) = Schur({1\\over p}M)$, where $p$ is the empirical sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2874df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_GD(M,Omega,r,maxIter = 1000,tol = 1e-3):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Chen, Ji, Xiaodong Li, and Zongming Ma. \"Nonconvex matrix completion with linearly parameterized factors.\" Journal of Machine Learning Research 23.207 (2022): 1-35.\n",
    "    .. [2] Sun, Ruoyu, and Zhi-Quan Luo. \"Guaranteed matrix completion via non-convex factorization.\" IEEE Transactions on Information Theory 62.11 (2016): 6535-6579.\n",
    "    .. [3] Chi, Yuejie. \"Low-rank matrix completion [lecture notes].\" IEEE Signal Processing Magazine 35.5 (2018): 178-181.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------   \n",
    "    M: array\n",
    "        Observed matrix.\n",
    "    Omega: array\n",
    "        Indicator matrix (0:observed;1:observed).\n",
    "    r: even integer\n",
    "        Desired rank.\n",
    "    maxIter: integer, optional\n",
    "        Maximum allowed iteration. Default is 1000.\n",
    "    tol: float, optional\n",
    "        Absolute tolerances. Default is 1e-3.\n",
    "    \n",
    "    Returns:\n",
    "    ----------   \n",
    "    M_rec: array\n",
    "        Recovered matrix.\n",
    "    \n",
    "    \"\"\"         \n",
    "    if(r%2 != 0):\n",
    "        raise ValueError(\"r must be an even number\")\n",
    "        \n",
    "    N = len(M);\n",
    "    \n",
    "    # objective function\n",
    "    obj = lambda A,B,M,p: 1/(2*p)*np.linalg.norm(Omega*(A@B.T-B@A.T-M),'fro')**2+1/4*np.linalg.norm(A.T@A-B.T@B,'fro')**2+1/4*np.linalg.norm(A.T@B+B.T@A,'fro')**2\n",
    "    \n",
    "    # empirical sample rate\n",
    "    p = sum(sum(Omega))/N**2;\n",
    "    \n",
    "    # Initialization\n",
    "    U,Q = scipy.linalg.schur((1/p)*M)\n",
    "    A = np.zeros((N,int(r/2)))\n",
    "    B = np.zeros((N,int(r/2)))\n",
    "    for i in range(int(r/2)):\n",
    "        if (U[2*i+1,2*i]>0):\n",
    "            A[:,i] = np.sqrt(U[2*i+1,2*i])*Q[:,2*i+1]\n",
    "            B[:,i] = np.sqrt(U[2*i+1,2*i])*Q[:,2*i]\n",
    "        elif (U[2*i+1,2*i]<0):\n",
    "            A[:,i] = np.sqrt(-U[2*i+1,2*i])*Q[:,2*i]\n",
    "            B[:,i] = np.sqrt(-U[2*i+1,2*i])*Q[:,2*i+1]\n",
    "    # gradient\n",
    "    for i in range(maxIter):\n",
    "        # gradient\n",
    "        gradA = 1/p*(Omega*(A@B.T-B@A.T-M)@B-Omega*(B@A.T-A@B.T+(-M).T)@B)+A@(A.T@A-B.T@B)+B@(B.T@A+A.T@B)\n",
    "        gradB = 1/p*(Omega*(B@A.T-A@B.T+(-M).T)@A-Omega*(A@B.T-B@A.T-M)@A)-B@(A.T@A-B.T@B)+A@(B.T@A+A.T@B)\n",
    "        # print(np.linalg.norm(gradB,'fro'))\n",
    "        # step size    \n",
    "        k = 1\n",
    "        while (obj(A-10**k*gradA,B-10**k*gradB,M,p)>obj(A,B,M,p)) & (k>=-20):\n",
    "            k = k-1\n",
    "            \n",
    "        # update\n",
    "        A = A - 10**k*gradA\n",
    "        B = B - 10**k*gradB\n",
    "        \n",
    "        # print information\n",
    "        if (i == 0) | (i%50 == 0) | (np.linalg.norm(np.concatenate((gradA,gradB)),'fro') < tol):\n",
    "            print(\"iter: %d, obj: %f \\n\"%(i,obj(A,B,M,p)))\n",
    "            \n",
    "        if (np.linalg.norm(np.concatenate((gradA,gradB)),'fro') < tol):\n",
    "            break\n",
    "    return A@B.T-B@A.T\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde6ee3",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f788e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokem = np.load(\"Data and Examples\\Data and Examples\\Pokemon\\F.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "92703345",
   "metadata": {},
   "outputs": [],
   "source": [
    "FM = pokem;\n",
    "N = len(FM);\n",
    "\n",
    "# Random observation matrix\n",
    "samp_rate = 0.5;  # sampling rate\n",
    "Omega = scipy.stats.bernoulli.rvs(size=N*N,p=samp_rate).reshape(N,N)\n",
    "\n",
    "for i in range(1,N):\n",
    "    for j in range(N-1):\n",
    "        Omega[j,i] = Omega[i,j];\n",
    "\n",
    "for i in range(N):\n",
    "    Omega[i,i] = 1;\n",
    "\n",
    "# observed matrix\n",
    "OM = FM*Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2329e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, obj: 139257.280283 \n",
      "\n",
      "iter: 50, obj: 13276.149406 \n",
      "\n",
      "iter: 100, obj: 9036.760947 \n",
      "\n",
      "iter: 150, obj: 6819.612569 \n",
      "\n",
      "iter: 200, obj: 5415.770732 \n",
      "\n",
      "iter: 250, obj: 4776.876209 \n",
      "\n",
      "iter: 300, obj: 4473.514263 \n",
      "\n",
      "iter: 350, obj: 4326.169054 \n",
      "\n",
      "iter: 400, obj: 4265.747965 \n",
      "\n",
      "iter: 450, obj: 4241.183495 \n",
      "\n",
      "iter: 500, obj: 4229.447780 \n",
      "\n",
      "iter: 550, obj: 4222.581896 \n",
      "\n",
      "iter: 600, obj: 4217.920836 \n",
      "\n",
      "iter: 650, obj: 4214.466524 \n",
      "\n",
      "iter: 700, obj: 4211.838288 \n",
      "\n",
      "iter: 750, obj: 4209.751027 \n",
      "\n",
      "iter: 800, obj: 4208.040959 \n",
      "\n",
      "iter: 850, obj: 4206.660305 \n",
      "\n",
      "iter: 900, obj: 4205.517141 \n",
      "\n",
      "iter: 950, obj: 4204.560401 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "M_rec = MC_GD(OM,Omega,16,maxIter = 1000,tol = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac2905",
   "metadata": {},
   "source": [
    "### Algorithm 2: Singular value thresholding\n",
    "\n",
    "This algotirhm is based on SVD.\n",
    "\n",
    "$\\operatorname{minimize} \\quad \\lambda\\|\\boldsymbol{X}\\|_*+\\frac{1}{2}\\left\\|\\mathcal{P}_{\\Omega}(\\boldsymbol{X})-\\mathcal{P}_{\\Omega}(\\boldsymbol{M})\\right\\|_F^2$\n",
    "\n",
    "Iterates\n",
    "$\\left\\{\\begin{array}{l}\\boldsymbol{X}^k=\\mathcal{D}_{\\lambda \\delta_{k-1}}\\left(\\boldsymbol{Y}^{k-1}\\right) \\\\ \\boldsymbol{Y}^k=\\boldsymbol{X}^k+\\delta_k P_{\\Omega}\\left(\\boldsymbol{M}-\\boldsymbol{X}^k\\right)\\end{array}\\right.$,\n",
    "\n",
    "where the soft-thresholding operator is defined as $\\left.\\mathcal{D}_\\tau(\\boldsymbol{X}):=\\boldsymbol{U D}_\\tau(\\boldsymbol{\\Sigma}) \\boldsymbol{V}^*,\\mathcal{D}_\\tau(\\boldsymbol{\\Sigma})=\\operatorname{diag}\\left(\\left\\{\\sigma_i-\\tau\\right)_{+}\\right\\}\\right)$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "95911a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_SVT(M,Omega,t=None,delta_t=2,maxIter=1000,tol=1e-2):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Cai, Jian-Feng, Emmanuel J. CandÃ¨s, and Zuowei Shen. \"A singular value thresholding algorithm for matrix completion.\" SIAM Journal on optimization 20.4 (2010): 1956-1982.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------   \n",
    "    M: array\n",
    "        Observed matrix.\n",
    "    Omega: array\n",
    "        Indicator matrix (0:observed;1:observed).\n",
    "    t: float,optimal\n",
    "        Singular value threshood. Default is 2*N.\n",
    "           \n",
    "    delta_t: float, optional\n",
    "        Step size. Default is 2. Try larger values if it's too slow. Try smaller values if it doesn't converge.\n",
    "    maxIter: integer, optional\n",
    "        Maximum allowed iteration. Default is 1000.\n",
    "    tol: float, optional\n",
    "        Absolute tolerances. Default is 1e-2.\n",
    "    \n",
    "    Returns:\n",
    "    ----------   \n",
    "    M_rec: array\n",
    "        Recovered matrix.\n",
    "    \n",
    "    \"\"\"  \n",
    "    \n",
    "    N = len(M)\n",
    "    if t == None:\n",
    "        t = 2*N\n",
    "        \n",
    "    X = Omega*M\n",
    "    Y = np.zeros((N,N))\n",
    "    iters = 0\n",
    "    epsilon= 1e+5\n",
    "    \n",
    "    while (epsilon>tol) & (iters<maxIter):\n",
    "        U,S,VT = np.linalg.svd(Y)\n",
    "        S = S - t\n",
    "        ind = S > 0\n",
    "        S = S*ind \n",
    "        X = U@np.diag(S)@VT;  \n",
    "        Y = Y + delta_t*Omega*(M-X)\n",
    "        epsilon= np.linalg.norm(Omega*(M-X),'fro')/np.linalg.norm(Omega*M,'fro')\n",
    "        iters = iters + 1\n",
    "        if (iters == 1) | (iters%50 == 0) | (epsilon < tol):\n",
    "            print(\"iter: %d, err: %f \\n\"%(iters,epsilon))\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb73932",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ff0c2fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, err: 1.000000 \n",
      "\n",
      "iter: 50, err: 0.054070 \n",
      "\n",
      "iter: 100, err: 0.037609 \n",
      "\n",
      "iter: 150, err: 0.028676 \n",
      "\n",
      "iter: 200, err: 0.023586 \n",
      "\n",
      "iter: 250, err: 0.019411 \n",
      "\n",
      "iter: 300, err: 0.016431 \n",
      "\n",
      "iter: 350, err: 0.014113 \n",
      "\n",
      "iter: 400, err: 0.012105 \n",
      "\n",
      "iter: 450, err: 0.010615 \n",
      "\n",
      "iter: 475, err: 0.009996 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "M_rec = MC_SVT(OM,Omega,delta_t = 2,maxIter = 1000, tol = 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
